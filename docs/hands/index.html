<!DOCTYPE HTML>
<html lang="ja">
<title>MediaPipe Hand</title>

<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#">
  <meta charset="UTF-8" name="viewport"
    content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta property="og:title" content="MediaPipe Hand" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="./ogp.jpg" />
  <meta name="description" content="GoogleのAIライブラリ「MediaPipe」を使ってみた。Handでは、カメラから手を認識してくれます。医療現場での利用を考えていきます。" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="MediaPipe Hand" />
  <meta name="twitter:image" content="./ogp.jpg" />
  <meta name="author" content="仁志">
  <meta name="keywords" content="MediaPipe, hand, Google, MediaPipe Hand" />
  <style>
    body{
      text-align: center;
    }
    
    video {
      clear: both;
      display: block;
      transform: rotateY(180deg);
      -webkit-transform: rotateY(180deg);
      -moz-transform: rotateY(180deg);
    }

    .videoView,
    .detectOnClick {
      position: relative;
      float: left;
      width: 48%;
      margin: 2% 1%;
      cursor: pointer;
    }

    .videoView p,
    .detectOnClick p {
      position: absolute;
      padding: 5px;
      background-color: #007f8b;
      color: #fff;
      border: 1px dashed rgba(255, 255, 255, 0.7);
      z-index: 2;
      font-size: 12px;
      margin: 0;
    }

    .highlighter {
      background: rgba(0, 255, 0, 0.25);
      border: 1px dashed #fff;
      z-index: 1;
      position: absolute;
    }

    .canvas {
      z-index: 1;
      position: absolute;
      pointer-events: none;
    }

    .output_canvas {
      transform: rotateY(180deg);
      -webkit-transform: rotateY(180deg);
      -moz-transform: rotateY(180deg);
    }
  </style>
</head>

<body>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>

  <body>
    <h1>MediaPipe Hand</h1>

    <div id="liveView" class="videoView">
      <button id="webcamButton" class="mdc-button mdc-button--raised">
        <span class="mdc-button__ripple"></span>
        <span class="mdc-button__label">ENABLE WEBCAM</span>
      </button>
      <div style="position: relative;">
        <video id="webcam" style="position: abso" autoplay playsinline></video>
        <canvas class="output_canvas" id="output_canvas" style="position: absolute; left: 0px; top: 0px;"></canvas>
      </div>
    </div>

    <script type="module">
      import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

      let handLandmarker = undefined;
      let runningMode = "IMAGE";
      const video = document.getElementById("webcam");
      const canvasElement = document.getElementById("output_canvas");
      const canvasCtx = canvasElement.getContext("2d");
      const enableWebcamButton = document.getElementById("webcamButton");
      let lastVideoTime = -1;
      let results = undefined;

      const createHandLandmarker = async () => {
        const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
        handLandmarker = await HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task`,
            delegate: "GPU"
          },
          runningMode: runningMode,
          numHands: 2
        });

      };
      createHandLandmarker();

      let predictWebcam = async () => {
        canvasElement.style.width = video.videoWidth;
        canvasElement.style.height = video.videoHeight;
        canvasElement.width = video.videoWidth;
        canvasElement.height = video.videoHeight;

        if (runningMode === "IMAGE") {
          runningMode = "VIDEO";
          await handLandmarker.setOptions({ runningMode: "VIDEO" });
        }
        let startTimeMs = performance.now();
        if (lastVideoTime !== video.currentTime) {
          lastVideoTime = video.currentTime;
          results = handLandmarker.detectForVideo(video, startTimeMs);
        }
        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        if (results.landmarks) {
          for (const landmarks of results.landmarks) {

            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: "#00FF00", lineWidth: 5 });
            drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });

          }
        }
        canvasCtx.restore();

        window.requestAnimationFrame(predictWebcam);

      }

      let enableCam = (event) => {
        if (!handLandmarker) {
          console.log("Wait! objectDetector not loaded yet.");
          return;
        }

        const constraints = {
          video: true
        };
        // Activate the webcam stream.
        navigator.mediaDevices.getUserMedia(constraints).then((stream) => {
          video.srcObject = stream;
          video.addEventListener("loadeddata", predictWebcam);
        });
      }

      enableWebcamButton.addEventListener("click", enableCam);

    </script>

  </body>

</html>